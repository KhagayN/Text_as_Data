install.packages(tm)
install.packages("tm")
install.packages("tm")
library(tm)
setwd("C:/Users/kevin/Documents/GitHub/Text_as_Data/cons")
df.tweets <- read.csv("bullying.csv", stringsAsFactors = FALSE)
setwd("C:/Users/kevin/Documents/GitHub/Text_as_Data/")
df.tweets <- read.csv("bullying.csv", stringsAsFactors = FALSE)
no_bullying <- paste(df.tweets$text[df.tweets$bullying_traces=="n"], collapse=" ")
yes_bullying <- paste(df.tweets$text[df.tweets$bullying_traces=="y"], collapse=" ")
groups <- VCorpus(VectorSource(c("No bullying" = no_bullying, "Yes bullying" = yes_bullying)))
groups <- tm_map(groups, content_transformer(tolower))
groups <- tm_map(groups, removePunctuation)
groups <- tm_map(groups, stripWhitespace)
dtm <- DocumentTermMatrix(groups)
dtm$dimnames$Docs = c("No bullying", "Yes bullying")
tdm <- t(dtm)
tdm <- as.matrix(weightTfIdf(tdm))
library(wordcloud)
comparison.cloud(tdm, max.words=100, colors=c("red", "blue"))
df <- df.tweets[df.tweets$type!="",]
df$type <- as.numeric(factor(df$bullying_traces))
training_break <- as.integer(0.9*nrow(df))
dtm       <- create_matrix(df$text, language="english", stemWords = TRUE,
weighting = weightTfIdf, removePunctuation = FALSE)
library(RTextTools)
dtm       <- create_matrix(df$text, language="english", stemWords = TRUE,
weighting = weightTfIdf, removePunctuation = FALSE)
df$text[1]
df$text[2]
df$text[3]
df$text[5]
container      <- create_container(dtm, t(df$type), trainSize=1:training_break,
testSize=training_break:nrow(df), virgin=FALSE)
View(tdm)
?create_container
?create_matrix
df.tweets$type <- as.numeric(factor(df.tweets$bullying_traces))
training_break <- as.integer(0.9*nrow(df))
dtm       <- create_matrix(.tweets$text, language="english", stemWords = TRUE,
weighting = weightTfIdf, removePunctuation = FALSE)
container      <- create_container(dtm, t(.tweets$type), trainSize=1:training_break,
testSize=training_break:nrow(df), virgin=FALSE)
df.tweets$type <- as.numeric(factor(df.tweets$bullying_traces))
training_break <- as.integer(0.9*nrow(df))
dtm       <- create_matrix(df.tweets$text, language="english", stemWords = TRUE,
weighting = weightTfIdf, removePunctuation = FALSE)
container      <- create_container(dtm, t(df.tweets$type), trainSize=1:training_break,
testSize=training_break:nrow(df), virgin=FALSE)
df.tweets <- read.csv("bullying.csv", stringsAsFactors = FALSE)
# Identify posts with and without bullying traces and create large documents
no_bullying <- paste(df.tweets$text[df.tweets$bullying_traces=="n"], collapse=" ")
yes_bullying <- paste(df.tweets$text[df.tweets$bullying_traces=="y"], collapse=" ")
# Create DTM and preprocess
groups <- VCorpus(VectorSource(c("No bullying" = no_bullying, "Yes bullying" = yes_bullying)))
groups <- tm_map(groups, content_transformer(tolower))
groups <- tm_map(groups, removePunctuation)
groups <- tm_map(groups, stripWhitespace)
dtm <- DocumentTermMatrix(groups)
## Label the two groups
dtm$dimnames$Docs = c("No bullying", "Yes bullying")
## Transpose matrix so that we can use it with comparison.cloud
tdm <- t(dtm)
## Compute TF-IDF transformation
tdm <- as.matrix(weightTfIdf(tdm))
## Display the two word clouds
library(wordcloud)
comparison.cloud(tdm, max.words=100, colors=c("red", "blue"))
###Let's train an SVM
df.tweets$type <- as.numeric(factor(df.tweets$bullying_traces))
training_break <- as.integer(0.9*nrow(df))
dtm       <- create_matrix(df.tweets$text, language="english", stemWords = TRUE,
weighting = weightTfIdf, removePunctuation = FALSE)
dtm       <- create_matrix(df.tweets$text, language="english", stemWords = FALSE,
weighting = weightTfIdf, removePunctuation = FALSE)
container      <- create_container(dtm, t(df.tweets$type), trainSize=1:training_break,
testSize=training_break:nrow(df), virgin=FALSE)
?cross_validate
?create_container
?cross_validate
?e1017
install.packages("e1017")
??e1017
cv.svm <- cross_validate(container, nfold=10, algorithm = 'SVM', kernel = 'linear')
cv.svm$meanAccuracy
prop.table(table(.tweets$type)) # baseline
prop.table(table(df.tweets$type)) # baseline
cv.svm <- cross_validate(container, nfold=10, algorithm = 'SVM', kernel = 'radial')
cv.svm$meanAccuracy
cv.svm$meanAccuracy
prop.table(table(df.tweets$type)) # baseline
container      <- create_container(dtm, t(df.tweets$type), virgin=FALSE)
container      <- create_container(dtm, t(df.tweets$type), trainSize=1:length(df.tweets$type),
virgin=FALSE)
cv.svm <- cross_validate(container, nfold=10, algorithm = 'SVM', kernel = 'linear')
cv.svm <- cross_validate(container, nfold=0, algorithm = 'SVM', kernel = 'linear')
cv.svm <- cross_validate(container, nfold=1, algorithm = 'SVM', kernel = 'linear')
cv.svm <- cross_validate(container, nfold=2, algorithm = 'SVM', kernel = 'linear')
?svm
cv.svm <- svm(container)
cv.svm <- cross_validate(container, nfold=2, algorithm = 'SVM', kernel = 'linear')
container      <- create_container(dtm, t(df.tweets$type), trainSize=1:training_break,
testSize=training_break:nrow(df), virgin=FALSE)
cv.svm <- cross_validate(container, nfold=10, algorithm = 'SVM', kernel = 'linear')
cv.svm <- cross_validate(container, nfold=10, algorithm = 'SVM', kernel = 'radial')
training_break <- as.integer(0.9*nrow(df.tweets))
dtm       <- create_matrix(df.tweets$text, language="english", stemWords = FALSE,
weighting = weightTfIdf, removePunctuation = FALSE)
###
container      <- create_container(dtm, t(df.tweets$type), trainSize=1:length(df.tweets$type),
virgin=FALSE)
cv.svm <- cross_validate(container, nfold=2, algorithm = 'SVM', kernel = 'linear')
container      <- create_container(dtm, t(df.tweets$type), trainSize=1:training_break,
testSize=training_break:nrow(df), virgin=FALSE)
###Let's train the model
cv.svm <- cross_validate(container, nfold=10, algorithm = 'SVM', kernel = 'linear')
cv.svm$meanAccuracy
prop.table(table(df.tweets$type)) # baseline
training_break <- as.integer(0.5*nrow(df.tweets))
dtm       <- create_matrix(df.tweets$text, language="english", stemWords = FALSE,
weighting = weightTfIdf, removePunctuation = FALSE)
###
container      <- create_container(dtm, t(df.tweets$type), trainSize=1:length(df.tweets$type),
virgin=FALSE)
cv.svm <- cross_validate(container, nfold=2, algorithm = 'SVM', kernel = 'linear')
training_break <- as.integer(0.5*nrow(df.tweets))
dtm       <- create_matrix(df.tweets$text, language="english", stemWords = FALSE,
weighting = weightTfIdf, removePunctuation = FALSE)
###
container      <- create_container(dtm, t(df.tweets$type), trainSize=1:training_break,
testSize=training_break:nrow(df), virgin=FALSE)
##validate
cv.svm <- cross_validate(container, nfold=2, algorithm = 'SVM', kernel = 'linear')
?GLMNET
??GLMNET
train<-dtm[1:training_break]
train<-dtm[1:training_break,]
model <- glm(df.tweets$type ~ .,family=binomial(link='logit'),data=train)
<-xxas.data.frame(train)
xx<-as.data.frame(train)
?dtm
?create_matrix
tabledf.tweets$type)
table(df.tweets$type)
table(df.tweets$type)
df.tweets <- read.csv("bullying.csv", stringsAsFactors = FALSE)
table(df.tweets$type)
df <- df.tweets[df.tweets$type!="",]
df$type <- as.numeric(factor(df$type))
training_break <- as.integer(0.9*nrow(df))
dtm       <- create_matrix(df$text, language="english", stemWords = FALSE,
weighting = weightTfIdf, removePunctuation = FALSE)
container      <- create_container(dtm, t(df$type), trainSize=1:training_break,
testSize=training_break:nrow(df), virgin=FALSE)
cv.svm <- cross_validate(container, nfolds=10, algorithm = 'SVM', kernel = 'linear')
cv.svm <- cross_validate(container, nfold=10, algorithm = 'SVM', kernel = 'linear')
cv.svm$meanAccuracy
prop.table(table(df$type)) # baseline
nyt.fb <- read.csv("./nyt-fb.csv", stringsAsFactors=FALSE)
head(nyt.fb$created_time)
str(nyt.fb)
total.resp <- nyt.fb$likes_count + nyt.fb$shares_count + nyt.fb$comments_count
quantile(total.resp, .95) # top 5% total share/count/comment total
viral <- total.resp > 10000
nyt.fb$viral <- viral
library(e1071)
install.packages("e1017")
?e1017
svm.viral <- svm(as.factor(viral) ~ month + hour + topic1 + topic2 + topic3 +
israel + trump + hillary + obama + terror + kill +
debate + sent.score, data=nyt.fb)
message <- removePunctuation(tolower(nyt.fb$message))
nyt.fb$israel <- grepl("israel", message)
nyt.fb$trump <- grepl("trump", message)
nyt.fb$hillary <- grepl("hillary", message)
nyt.fb$obama <- grepl("barack|obama", message)
nyt.fb$terror <- grepl("terror|isis|isil|qaeda", message)
nyt.fb$kill <- grepl("kill|murder|shot", message)
nyt.fb$debate <- grepl("debat", message)
svm.viral <- svm(as.factor(viral) ~ month + hour  +
israel + trump + hillary + obama + terror + kill +
debate  , data=nyt.fb)
message <- removePunctuation(tolower(nyt.fb$message))
nyt.fb$israel <- grepl("israel", message)
nyt.fb$trump <- grepl("trump", message)
nyt.fb$hillary <- grepl("hillary", message)
nyt.fb$obama <- grepl("barack|obama", message)
nyt.fb$terror <- grepl("terror|isis|isil|qaeda", message)
nyt.fb$kill <- grepl("kill|murder|shot", message)
nyt.fb$debate <- grepl("debat", message)
cv.svm <- cross_validate(as.factor(viral) ~ month + hour  +
israel + trump + hillary + obama + terror + kill +
debate  , data=nyt.fb, nfold=5, algorithm = 'SVM', kernel = 'linear')
?cross_validate
?create_container
glm.viral <- glm(as.factor(viral) ~ month + hour  +
israel + trump + hillary + obama + terror + kill +
debate , data=nyt.fb, family=binomial(logit))
head(nyt.fb$created_time)
month <- substr(nyt.fb$created_time, 6, 7)
hour <- substr(nyt.fb$created_time, 12, 13)
nyt.fb <- data.frame(nyt.fb, month, hour)
glm.viral <- glm(as.factor(viral) ~ month + hour  +
israel + trump + hillary + obama + terror + kill +
debate , data=nyt.fb, family=binomial(logit))
nyt.fb$viral <- as.numeric(factor(nyt.fb$viral))
training_break <- as.integer(0.9*nrow(nyt.fb))
dtm       <- create_matrix(nyt.fb$message, language="english", stemWords = FALSE,
weighting = weightTfInyt.fb, removePunctuation = FALSE)
dtm       <- create_matrix(nyt.fb$message, language="english", stemWords = FALSE,
weighting = weightTfIdf, removePunctuation = FALSE)
container      <- create_container(dtm, t(nyt.fb$type), trainSize=1:training_break,
testSize=training_break:nrow(nyt.fb), virgin=FALSE)
cv.svm$meanAccuracy
cv.svm <- cross_validate(container, nfold=2, algorithm = 'SVM', kernel = 'linear')
cv.svm$meanAccuracy
prop.table(table(nyt.fb$viral))
glm.viral
lm.resp <- lm(I(log(shares_count + likes_count + comments_count)) ~ month +
hour + israel + trump +
hillary + obama + terror + kill + debate + , data=nyt.fb)
lm.resp <- lm(I(log(shares_count + likes_count + comments_count)) ~ month +
hour + israel + trump +
hillary + obama + terror + kill + debate  , data=nyt.fb)
lm.resp
?glm
glm.viral$fitted.values
glm.viral$residuals
glm.viral$predicted_values
glm.viral$fitted.values
round(glm.viral$fitted.values)
table(round(glm.viral$fitted.values))
prop.table(table(nyt.fb$viral))
cv.svm$meanAccuracy
library(tm)
setwd("C:/Users/kevin/Documents/GitHub/Text_as_Data/")
df.tweets <- read.csv("bullying.csv", stringsAsFactors = FALSE)
# Identify posts with and without bullying traces and create large documents
no_bullying <- paste(df.tweets$text[df.tweets$bullying_traces=="n"], collapse=" ")
yes_bullying <- paste(df.tweets$text[df.tweets$bullying_traces=="y"], collapse=" ")
# Create DTM and preprocess
groups <- VCorpus(VectorSource(c("No bullying" = no_bullying, "Yes bullying" = yes_bullying)))
groups <- tm_map(groups, content_transformer(tolower))
groups <- tm_map(groups, removePunctuation)
groups <- tm_map(groups, stripWhitespace)
dtm <- DocumentTermMatrix(groups)
## Label the two groups
dtm$dimnames$Docs = c("No bullying", "Yes bullying")
## Transpose matrix so that we can use it with comparison.cloud
tdm <- t(dtm)
## Compute TF-IDF transformation
tdm <- as.matrix(weightTfIdf(tdm))
## Display the two word clouds
library(wordcloud)
comparison.cloud(tdm, max.words=100, colors=c("red", "blue"))
###Let's train an SVM
df.tweets$type <- as.numeric(factor(df.tweets$bullying_traces))
training_break <- as.integer(0.9*nrow(df.tweets))
library(RTextTools)
?create_matrix
dtm       <- create_matrix(df.tweets$text, language="english", stemWords = FALSE,
weighting = weightTfIdf, removePunctuation = FALSE)
#####Make it all in-sample
container      <- create_container(dtm, t(df.tweets$type), trainSize=1:length(df.tweets$type),
virgin=FALSE)
##train the model
cv.svm <- cross_validate(container, nfold=2, algorithm = 'SVM', kernel = 'linear')
?cross_validate
########
container      <- create_container(dtm, t(df.tweets$type), trainSize=1:training_break,
testSize=training_break:nrow(df), virgin=FALSE)
###Let's train the model
cv.svm <- cross_validate(container, nfold=2, algorithm = 'SVM', kernel = 'linear')
##validate
cv.svm$meanAccuracy
prop.table(table(df.tweets$type)) # baseline
####################Let's try again with a different kernel
cv.svm <- cross_validate(container, nfold=10, algorithm = 'SVM', kernel = 'radial')
#################What if we try with different % test/train
training_break <- as.integer(0.5*nrow(df.tweets))
dtm       <- create_matrix(df.tweets$text, language="english", stemWords = FALSE,
weighting = weightTfIdf, removePunctuation = FALSE)
?create_matrix
###
container      <- create_container(dtm, t(df.tweets$type), trainSize=1:training_break,
testSize=training_break:nrow(df), virgin=FALSE)
cv.svm <- cross_validate(container, nfold=2, algorithm = 'SVM', kernel = 'linear')
cv.svm$meanAccuracy
prop.table(table(df.tweets$type)) # baseline
#############Let's do this multinomially
df.tweets <- read.csv("bullying.csv", stringsAsFactors = FALSE)
table(df.tweets$type)
##filter to only those that are bullying
df <- df.tweets[df.tweets$type!="",]
df$type <- as.numeric(factor(df$type))
training_break <- as.integer(0.9*nrow(df))
dtm       <- create_matrix(df$text, language="english", stemWords = FALSE,
weighting = weightTfIdf, removePunctuation = FALSE)
container      <- create_container(dtm, t(df$type), trainSize=1:training_break,
testSize=training_break:nrow(df), virgin=FALSE)
?create_container
cv.svm <- cross_validate(container, nfold=5, algorithm = 'SVM', kernel = 'linear')
cv.svm$meanAccuracy
prop.table(table(df$type)) # baseline
nyt.fb <- read.csv("./nyt-fb.csv", stringsAsFactors=FALSE)
str(nyt.fb)
##create variables for month and hour
head(nyt.fb$created_time)
month <- substr(nyt.fb$created_time, 6, 7)
hour <- substr(nyt.fb$created_time, 12, 13)
nyt.fb <- data.frame(nyt.fb, month, hour)
##########decide what constitutes ''viral"
total.resp <- nyt.fb$likes_count + nyt.fb$shares_count + nyt.fb$comments_count
##look at the extreme of the distribution
quantile(total.resp, .95)
viral <- total.resp > 10000
nyt.fb$viral <- viral
########### For the purposes of not destroying my laptop, let's choose a set of features
nyt.fb$viral <- as.numeric(factor(nyt.fb$viral))
training_break <- as.integer(0.9*nrow(nyt.fb))
dtm       <- create_matrix(nyt.fb$message, language="english", stemWords = FALSE,
weighting = weightTfIdf, removePunctuation = FALSE)
container      <- create_container(dtm, t(nyt.fb$type), trainSize=1:training_break,
testSize=training_break:nrow(nyt.fb), virgin=FALSE)
cv.svm <- cross_validate(container, nfold=2, algorithm = 'SVM', kernel = 'linear')
cv.svm$meanAccuracy
prop.table(table(nyt.fb$viral))
############relative to logistic regression
message <- removePunctuation(tolower(nyt.fb$message))
nyt.fb$israel <- grepl("israel", message)
nyt.fb$trump <- grepl("trump", message)
nyt.fb$hillary <- grepl("hillary", message)
nyt.fb$obama <- grepl("barack|obama", message)
nyt.fb$terror <- grepl("terror|isis|isil|qaeda", message)
nyt.fb$kill <- grepl("kill|murder|shot", message)
nyt.fb$debate <- grepl("debat", message)
# regression example
glm.viral <- glm(as.factor(viral) ~ month + hour  +
israel + trump + hillary + obama + terror + kill +
debate , data=nyt.fb, family=binomial(logit))
###
library(devtools)
devtools::install_github("kbenoit/quanteda")
devtools::install_github("kbenoit/quantedaData")
# install.packages("data.table")
library(dplyr)
library(data.table)
library(quanteda)
##set WD
setwd("C:/Users/kevin/Dropbox/Benoit_Spirling_Readability/")
load("data_CF/snippetData.RData")
df<-SOTU_comparisons_all
indices<-vector()
for(i in 1:650){
index<-which.max(abs(df$FRE1 - df$FRE2))
indices<-c(indices,index )
df<-df[-index,]
}
length(which(SOTU_comparisons_all$FRE1<0))
gold_df<-SOTU_comparisons_all[indices,]
##begin easier as all snippet 1
gold_df$easier_gold<-rep(1,650)
##just compare FREs, replace if FRE2 > FRE1 (and 2 is easier than 1)
for(i in 1:650){
if(gold_df$FRE2[i]>gold_df$FRE1[i]){
gold_df$easier_gold[i]<-2
}
}
setwd("C:/Users/kevin/Dropbox/Benoit_Spirling_Readability/data_CF")
write.csv(gold_df, file = "SOTU_comparisons_gold_3_11.csv", row.names = FALSE)
setwd("C:/Users/kevin/Dropbox/Benoit_Spirling_Readability/")
load("data_CF/snippetData.RData")
load("data_CF/SOTU_Comparisons_All")
load("data_CF/SOTU_Comparisons_All.Rdata")
df<-SOTU_comparisons_all
indices<-vector()
indices<-vector()
###grab the biggest difference, delete the row, then save the index
for(i in 1:650){
index<-which.max(abs(df$FRE1 - df$FRE2))
indices<-c(indices,index )
df<-df[-index,]
}
length(which(SOTU_comparisons_all$FRE1<0))
gold_df<-SOTU_comparisons_all[indices,]
##begin easier as all snippet 1
gold_df$easier_gold<-rep(1,650)
##just compare FREs, replace if FRE2 > FRE1 (and 2 is easier than 1)
for(i in 1:650){
if(gold_df$FRE2[i]>gold_df$FRE1[i]){
gold_df$easier_gold[i]<-2
}
}
setwd("C:/Users/kevin/Dropbox/Benoit_Spirling_Readability/data_CF")
write.csv(gold_df, file = "SOTU_comparisons_gold_3_11.csv", row.names = FALSE)
load("data_CF/SOTU_Comparisons_All.Rdata")
setwd("C:/Users/kevin/Dropbox/Benoit_Spirling_Readability/")
load("data_CF/SOTU_Comparisons_All.Rdata")
df<-SOTU_comparisons_all
for(i in 1:650){
index<-which.max(abs(df$FRE1 - df$FRE2))
indices<-c(indices,index )
df<-df[-index,]
}
length(which(SOTU_comparisons_all$FRE1<0))
gold_df<-SOTU_comparisons_all[indices,]
##begin easier as all snippet 1
gold_df$easier_gold<-rep(1,650)
##just compare FREs, replace if FRE2 > FRE1 (and 2 is easier than 1)
for(i in 1:650){
if(gold_df$FRE2[i]>gold_df$FRE1[i]){
gold_df$easier_gold[i]<-2
}
}
setwd("C:/Users/kevin/Dropbox/Benoit_Spirling_Readability/data_CF")
write.csv(gold_df, file = "SOTU_comparisons_gold_3_11.csv", row.names = FALSE)
##set WD
setwd("C:/Users/kevin/Dropbox/Benoit_Spirling_Readability/")
load("data_CF/SOTU_Comparisons_All.Rdata")
#make copy
df<-SOTU_comparisons_all
indices<-vector()
###grab the biggest difference, delete the row, then save the index
for(i in 1:650){
index<-which.max(abs(df$FRE1 - df$FRE2))
indices<-c(indices,index )
df<-df[-index,]
}
length(which(SOTU_comparisons_all$FRE1<0))
gold_df<-SOTU_comparisons_all[indices,]
##begin easier as all snippet 1
gold_df$easier_gold<-rep(1,650)
##just compare FREs, replace if FRE2 > FRE1 (and 2 is easier than 1)
for(i in 1:650){
if(gold_df$FRE2[i]>gold_df$FRE1[i]){
gold_df$easier_gold[i]<-2
}
}
setwd("C:/Users/kevin/Dropbox/Benoit_Spirling_Readability/data_CF")
write.csv(gold_df, file = "SOTU_comparisons_gold_3_11_a.csv", row.names = FALSE)
library(httr)
library(streamR)
library(devtools)
library(streamR)
library(Rcpp)
library(ROAuth)
library(quanteda)
library(smappR)
library(dplyr)
library(twitteR)
library(base64enc)
library(jsonlite)
library(stringr)
library(quanteda)
library(lubridate)
library(foreign)
setwd("C:/Users/Kevin/Dropbox/Dis Sem/")
load("C:/Users/Kevin/Dropbox/credentials/oauth_token15.Rdata")
?getFollowers
getFollowers(realDonaldTrump, oauth_folder="C:/Users/Kevin/Dropbox/credentials/", sleep=30)
T_followers<-getFollowers("realDonaldTrump", oauth_folder="C:/Users/Kevin/Dropbox/credentials/", sleep=30)
T_followers
T_followers<-getFollowers("realDonaldTrump", oauth_folder="C:/Users/Kevin/Dropbox/credentials/", sleep=30)
T_followers
load("C:/Users/kevin/Dropbox/astoria/peru_final.RData")
setwd("C:/Users/Kevin/Dropbox/astoria/")
write.csv(all_labeled_tweets, "aa_peru_tweets_raw.csv")
write.csv(all_labeled_tweets_df, "aa_peru_tweets_raw.csv")
raw_df<-all_labeled_tweets_df[, 1]
raw_df<-all_labeled_tweets_df[, 1:43]
write.csv(raw_df, "aa_peru_tweets_raw.csv")
library(RTextTools)
?cross_validate
setwd("C:/Users/kevin/Documents/GitHub/Text_as_Data/dickens_austen")
files <- list.files( full.names=TRUE)
text <- lapply(files, readLines)
texts<-unlist(lapply(text, function(x) paste(x, collapse = " ")))
head(texts[[1]], n=100)
tail(texts[[1]], n=500)
texts[[1]]<-head(texts[[1]], -366)
texts[[1]]<-tail(texts[[1]], -29)
texts[[2]]<-head(texts[[2]], -366)
texts[[2]]<-tail(texts[[2]], -29)
texts[[3]]<-head(texts[[3]], -366)
texts[[3]]<-tail(texts[[3]], -29)
texts[[4]]<-head(texts[[4]], -366)
texts[[4]]<-tail(texts[[4]], -29)
texts[[5]]<-head(texts[[5]], -366)
texts[[5]]<-tail(texts[[5]], -29)
texts[[6]]<-head(texts[[6]], -360)
texts[[6]]<-tail(texts[[6]], -33)
texts[[7]]<-head(texts[[7]], -363)
texts[[7]]<-tail(texts[[7]], -29)
texts[[8]]<-head(texts[[8]], -363)
texts[[8]]<-tail(texts[[8]], -29)
texts[[9]]<-head(texts[[9]], -363)
texts[[9]]<-tail(texts[[9]], -29)
texts[[10]]<-head(texts[[10]], -363)
texts[[10]]<-tail(texts[[10]], -29)
texts[[11]]<-head(texts[[11]], -363)
texts[[1]]
head(texts[[1]], n=100)
head(texts[[1]], n=100)
files <- list.files( full.names=TRUE)
text <- lapply(files, readLines)
head(text[[1]], n=100)
setwd("C:/Users/kevin/Documents/GitHub/Text_as_Data/dickens_austen")
files <- list.files( full.names=TRUE)
texts <- lapply(files, readLines)
files<-unlist(files)
files<-gsub("./", "", files )
files<-gsub(".txt", "", files )
